{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSCI 303\n",
    "# Introduction to Data Science\n",
    "<p/>\n",
    "\n",
    "## Decision Trees (and Random Forests)\n",
    "\n",
    "![](https://ars.els-cdn.com/content/image/1-s2.0-S0031320310003973-gr1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "Decision trees are a sequence of **if-else branches** in a tree structure, in which  \n",
    "each if-else decision is represented by a branch split, and **final decisions  \n",
    "are repesented by leaves** (multiple leaves may give the same decision).\n",
    "\n",
    "**Pros**\n",
    "- Can be use for either classification or regression\n",
    "- Handles categorical features naturally, and also handles numerical (continuous valued) features\n",
    "- Easily interpretable, and visualizable\n",
    "- Easy to train (with suboptimal methods)\n",
    " \n",
    "**Cons**\n",
    "- Can be prone to overfitting\n",
    "- Not a natural approach for continues-valued input and outputs (typical regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: a Titanic Survivor decision tree\n",
    "\n",
    "Branch splits are decision points based on a single feature. Leaves are the  \n",
    "final decision (classification or regression value) for the relevant path  \n",
    "through the tree.\n",
    "\n",
    "The figures under the leaves show the probability of survival (at that leaf)  \n",
    "and the percentage of observations in the leaf (sums to 100 over all leaves).\n",
    "\n",
    "\"sipbsp\" is number of spouses or siblings aboard.\n",
    "\n",
    "![Titanic decision tree](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the case of only two features and binary classification, we can create alternative visualizations of the information in a trained decision tree\n",
    "\n",
    "The data below are samples that indicate whether or not a patient developed kyphosis (convex spine curvature) after surgery for that same condition (target), with predictor features including patient age and the vertebra at which surgery was started.\n",
    "\n",
    "- Middle: A decision tree partitions the feature space to create a likelihood map\n",
    "- Right: A map of the decision partitions, overlayed with the training samples\n",
    "\n",
    "![kyphosis](https://upload.wikimedia.org/wikipedia/commons/2/25/Cart_tree_kyphosis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a decision tree\n",
    "\n",
    "Training is typically a top down process:\n",
    "- At each node (branch split), use some metric to determine which feature accommodates the \"best\" split.\n",
    " - E.g., **Gini impurity** metric\n",
    "- After the split, for each of the children branches we either:\n",
    " - Declare the child a leaf, and assign all samples that reach that leaf to a class or regression value.\n",
    " - Do another branch split, using the same process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An (invented) branch split feature selection example\n",
    "\n",
    "We want to **predict whether smartphone buyer will buy an iPhone or and Android phone**.  \n",
    "We have two **features**:\n",
    "- Person has **Bachelor's degree** or higher (yes/no)\n",
    "- Person prefers **square pizzas** to **circular pizzas** (square/round)\n",
    "\n",
    "Extreme numbers, just for saliency...\n",
    "- 50% of iPhone users prefer square pizza. Same for Android users  \n",
    "- 90% of people with a Bachelor's degree are iPhone owners  \n",
    "- 90% of people without a Bachelor's degree are Android owners\n",
    "\n",
    "**Q: Should our first node split based on Bachelor's degree or based on pizza preference?**\n",
    "- Splitting based on **pizza** gives two children nodes, with iPhone/Android **distributions no better than the parent node.**\n",
    "- Splitting based on **degree** gives childen nodes with iPhone/Android **distributions that are highly \"homogenous\"** (one child node is mostly iPhone owners, the other mostly Android owners).\n",
    "\n",
    "**So, we should make our first split is based on Bachelor's degree!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a tree with sklearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get necessary packages\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz # needed to visualize trained decision tree\n",
    "from sklearn.tree import export_graphviz # needed to visualize trained decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "# Set up for plotting\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Read in our data and glance at its formatting\n",
    "df = pd.read_csv('titanic_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare our data for viable input to the sklearn decision tree model\n",
    "\n",
    "# Convert categorical features into Boolean/binary dummy features\n",
    "df_dummy = pd.get_dummies(df, columns=['Pclass', 'Sex'])\n",
    "\n",
    "# Remove redundant male or female feature, and unneeded Name feature\n",
    "df_dummy = df_dummy.drop(columns=['Sex_male', 'Name'])\n",
    "df_dummy.head()\n",
    "\n",
    "# Create train/test data sets\n",
    "X = df_dummy.drop(columns=['Survived'])\n",
    "y = df_dummy['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we'll train a model with low depth\n",
    "We will set: ```max_depth==2```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier object, and train it.\n",
    "titanic_tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "titanic_tree.fit(X, y)\n",
    "\n",
    "# Visualize the tree\n",
    "from IPython.display import display\n",
    "columns = list(X_train.columns)\n",
    "display(graphviz.Source(export_graphviz(titanic_tree, feature_names=columns, class_names=True, out_file=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll train a model with slightly higher depth, but limit the minimum leaf size\n",
    "We will set: ```min_samples_split==150```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our model\n",
    "titanic_tree = DecisionTreeClassifier(max_depth=4, min_samples_split=150, random_state=0)\n",
    "titanic_tree.fit(X, y)\n",
    "\n",
    "# Visualize the tree\n",
    "from IPython.display import display\n",
    "columns = list(X_train.columns)\n",
    "display(graphviz.Source(export_graphviz(titanic_tree, feature_names=columns, class_names=True, out_file=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train models over a range of depths, and score them with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's train models over a range of depths, and score them with the test set\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "max_max = 20\n",
    "max_depth = np.arange(1, max_max)\n",
    "\n",
    "for depth in max_depth:\n",
    "    # Build model and train\n",
    "    titanic_tree = DecisionTreeClassifier(max_depth=depth, random_state=0) # call DecisionTreeClassifier to construct our tree\n",
    "    titanic_tree.fit(X_train, y_train)\n",
    "\n",
    "    # Test\n",
    "    acc_scores.append(titanic_tree.score(X_test, y_test))\n",
    "    y_test_hat = titanic_tree.predict(X_test)\n",
    "    f1_scores.append(f1_score(y_test, y_test_hat))\n",
    "\n",
    "# Print values for best test score\n",
    "ix_best = np.argmax(acc_scores) # the max score in acc\n",
    "print('Best accuracy score is %0.3f, for max_depth=%d' % (acc_scores[ix_best], max_depth[ix_best]))\n",
    "ix_best = np.argmax(f1_scores) # the max score in f1\n",
    "print('Best F1-score is %0.3f, for max_depth=%d' % (f1_scores[ix_best], max_depth[ix_best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(max_depth, acc_scores, 'o-', label='Accuracy')\n",
    "plt.plot(max_depth, f1_scores, 'o-', label='F1')\n",
    "plt.xlabel('Maximum tree depth')\n",
    "plt.ylabel('Score')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees are sometimes not the best choice for regression\n",
    "\n",
    "- Granularity of binned feature regions leads to over and underfitting\n",
    "\n",
    "![Decision tree regression](https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees: Recap\n",
    "\n",
    "**Pros**\n",
    "- Can be use for either classification or regression\n",
    "- Handles categorical features naturally, and also handles numerical (continuous valued) features\n",
    "- Easily interpretable, and visualizable\n",
    "- Easy to train (with suboptimal methods)\n",
    " \n",
    "**Cons**\n",
    "- Can be prone to overfitting (too deep, and/or too many leaves)\n",
    "- Not a natural approach for continues-valued input and outputs (typical regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next time\n",
    "\n",
    "**Random Forests: Ensembles of decision trees**\n",
    "- Performance improvements over individual decision trees\n",
    "- Ability to assess feature importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
