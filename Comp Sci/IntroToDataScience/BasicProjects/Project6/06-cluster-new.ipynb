{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 6 : Clustering\n",
    "- Name:\n",
    "- Date: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Description\n",
    "\n",
    "Practice clustering on a using the well known and very popular `Iris` Dataset! The Iris flower data set is fun for learning supervised classification algorithms, and is known as a difficult case for unsupervised learning. \n",
    "https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html\n",
    "<br><br>Yes, there are many examples out there, but see if you can do it yourself :). We can easily hypothesize on how many clusters would yield the best result, so let us prove it through a simple experiment that you could repeat with additional data sets.\n",
    "\n",
    "### Grading\n",
    "\n",
    "For grading purposes, we will clear all outputs from all your cells and then run them all from the top.  Please test your notebook in the same fashion before turning it in.\n",
    "\n",
    "### Submitting Your Solution\n",
    "\n",
    "To submit your notebook, first clear all the cells (this won't matter too much this time, but for larger data sets in the future, it will make the file smaller).  Then use the File->Download As->Notebook to obtain the notebook file.  Finally, submit the notebook file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Data Generation (5 points)\n",
    "Reference for more information: Chapter 5.11 K-Means in the online course book.\n",
    "\n",
    "1. Load the `iris` dataset and separate into `X` and `y` variables (our ground truth labels will just be used for visualization).\n",
    "2. Write a hypothesis on how many clusters will yield the best labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hypothesis**(Edit this cell)\n",
    "> I think that 3 clusters will yield the best labeling accuracy because there are three classes of data.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Data exploration (10 points)\n",
    "\n",
    "This is the step where you would normally conduct any needed preprocessing, data wrangling, and investigation of the data.\n",
    "<br>**Note:** `print(iris.DESCR)` prints the iris dataset description, provided you loaded it into a variable named `iris`\n",
    "\n",
    "a. Using your skills from previous projects, provide code below to produce answers to the following questions (edit this cell with your answers): \n",
    "\n",
    "    1. How many features are provided?\n",
    "        ->4\n",
    "    2. How many total observations?\n",
    "        ->150\n",
    "    3. How many different labels are included, what are they called, and is it a balanced dataset with the same number of observations for each class?\n",
    "        ->3 labels are included, which are called Setosa, Versicolour, and Virginica. The dataset is balanced with 50 observations for each class.\n",
    "b. Create a 2D or 3D scatter plot of two or three of the features and use the y labels for color coding. Do not reduce the data or number of features in any way (you will do this by applying PCA in problem 5).\n",
    "\n",
    "c. Since clusters can be influenced by the magnitudes of the variables, scale the feature data and plot a histogram of the transformed feature data (think about if you should use the min-max, standard scaler, or normalizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "print(\"Number of Features: {}\".format(len(iris.feature_names)))\n",
    "print(\"Number of Observations: {}\".format(len(X)))\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "colormp = ListedColormap(['r','b','g'])\n",
    "\n",
    "plt.scatter(X[\"petal length (cm)\"], X[\"petal width (cm)\"], c=y, cmap=colormp)\n",
    "plt.xlabel(\"petal length (cm)\")\n",
    "plt.ylabel(\"petal width (cm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. Scale the data (think about if you should use the min-max, standard scaler, or normalizer)\n",
    "\n",
    "normalized = Normalizer()\n",
    "normalized.fit(X)\n",
    "\n",
    "X_norm=normalized.transform(X)\n",
    "plt.hist(X)\n",
    "plt.title(\"Histogram of transformed data\")\n",
    "plt.xlabel(\"BinNumber\")\n",
    "plt.ylabel(\"BinCount\")\n",
    "plt.legend(X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Unsupervised Learning - Clustering (15 points)\n",
    "Conduct clustering experiments with one of algorithms discussed in class (e.g., k-means) for number of clusters k = 2-10. Create another 2D or 3D scatter plot utilizing the <b>cluster assignments</b> for color coding (this output can be a plot for each of the values of k or just one final plot using the value of k from your best Silhouette result obtained in Problem 4 below).  \n",
    "\n",
    "#### Steps:\n",
    "Repeat for each value of k (maybe a loop here would be appropriate):\n",
    "1. Create model object\n",
    "2. Train or fit the model\n",
    "3. Predict cluster assignments\n",
    "4. Calculate Silhouette width (see Problem 4)\n",
    "4. Plot points color coded by class labels predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Placing this here because I keep getting spammed with warnings that are specific to my computer\n",
    "'''\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "krange = np.arange(2,11,1)\n",
    "silhouette_widths = [0.0]*len(krange)\n",
    "bestSilhouette = 0.0\n",
    "bestY_hat = y\n",
    "for k in krange:\n",
    "    model = KMeans(k)\n",
    "    model.fit(X)\n",
    "    y_hat = model.predict(X)\n",
    "    silhouette_widths[k-2] = silhouette_score(X, y_hat)\n",
    "    if(silhouette_widths[k-2] > bestSilhouette): \n",
    "        bestY_hat = y_hat\n",
    "        bestSilhouette = silhouette_widths[k-2]\n",
    "\n",
    "plt.scatter(X[\"petal length (cm)\"], X[\"petal width (cm)\"], c=bestY_hat, cmap=colormp)\n",
    "plt.xlabel(\"petal length (cm)\")\n",
    "plt.ylabel(\"petal width (cm)\")\n",
    "plt.show()\n",
    "print()\n",
    "print(\"k(n_cluster) range = \\n{}\\n\".format(krange))\n",
    "print(\"silhouette width scores = \\n{}\\n\".format(silhouette_widths))\n",
    "print(\"Best silhouette width score = {}\".format(bestSilhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Evaluate results (20 points)\n",
    "\n",
    "As we have discussed, validating an usupervised problem is difficult. There is a metric that can be used to determine the density or separation of cluster assignments, called Silhouette width. In this step, perform analysis of results using the above `k = 2-10` and compute the Silhouette width (Hint: possibly you can just add code to your loop in problem 3 and store the results in a list of values). \n",
    "\n",
    "Scikit Learn has a great example for Silhouette analysis [here](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "\n",
    "1. For each k (k = 2-10), what are the Silhouette width values?\n",
    " \n",
    "\n",
    "2. Discuss if your best number of clusters (highest Silhouette width value) matches your hypothesis from Problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(krange)):\n",
    "    print(\"k value = {},  Silhouette width = {}\\n\".format(krange[i], silhouette_widths[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4B\n",
    "\"\"\"\n",
    "The best number of clusters in terms of silhouette width does not match my hypothesis from problem 1. Which was that\n",
    "3 clusters would be the best number of clusters, but I think that is because we are currently taking in too many features\n",
    "for processing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 (15 points): Principal Component Analysis (PCA)\n",
    "PCA is the most popular form of dimensionality reduction, which basically, rotates and transforms the data into a new subspace, such that the resultant matrix has:\n",
    "- Most relevance (variation) now associated with first feature\n",
    "- Second feature gets the next most, etc.\n",
    "#### Steps:\n",
    "1. Reduce the feature data (X) using PCA\n",
    "2. Repeat the same experiment from problem 3 above (remember your plots are now the 1st, 2nd, and possibly 3rd principal component vs. the raw feature data like before).\n",
    "3. Compare and contrast results to those from previous/non-PCA problems; does it perform better/worse/same? Provide discussion below (this could vary, depending on setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering with PCA\n",
    "nComponents = 2\n",
    "pca = PCA(n_components = nComponents)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "silhouette_widths_pca = [0.0]*len(krange)\n",
    "bestSilhouette_pca = 0.0\n",
    "bestY_hat_pca = y\n",
    "for k in krange:\n",
    "    model = KMeans(k)\n",
    "    model.fit(X_pca)\n",
    "    y_hat = model.predict(X_pca)\n",
    "    silhouette_widths_pca[k-2] = silhouette_score(X_pca, y_hat)\n",
    "    if(silhouette_widths[k-2] > bestSilhouette_pca): \n",
    "        bestY_hat_pca = y_hat\n",
    "        bestSilhouette_pca = silhouette_widths_pca[k-2]\n",
    "\n",
    "plt.scatter(X[\"petal length (cm)\"], X[\"petal width (cm)\"], c=bestY_hat_pca, cmap=colormp)\n",
    "plt.xlabel(\"petal length (cm)\")\n",
    "plt.ylabel(\"petal width (cm)\")\n",
    "plt.show()\n",
    "print()\n",
    "print(\"k(n_cluster) range = \\n{}\\n\".format(krange))\n",
    "print(\"silhouette width scores = \\n{}\\n\".format(silhouette_widths_pca))\n",
    "print(\"Best silhouette width score = {}\".format(bestSilhouette_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss new results**(Edit this cell)\n",
    "> Reducing the number of features via PCA appears to have barely improved the silhouette width score, with pca getting 70.5% and non-pca getting 68.1%. Looking back I now realize 2 clusters is the better number because the data is clustered into 2 main groups with one of those groups containing two classes of data. Although the issue with only using 2 clusters is that you lose one class completely, so clearly something is missing in our method.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Finished! Treat yourself by taking this questionnaire\n",
    "### Questionnaire\n",
    "1) How long did you spend on this assignment?\n",
    "<br><br>\n",
    "    About 2-3 hours wasn't really paying attention to when I started and I have gotten up to do things around the house frequently.\n",
    "<br><br>\n",
    "2) What did you like about it? What did you not like about it?\n",
    "<br><br>\n",
    "     I found the dataset interesting, and I found it interesting how clustering fails to work fully with this dataset. At least the clustering algorithm we are currently using.\n",
    "<br><br>\n",
    "3) Did you find any errors or is there anything you would like changed?\n",
    "<br><br>\n",
    "    None that I can think of.\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
